{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q.1. What is a parameter?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "  A parameter is an internal variable that a machine learning model learns during training. These parameters define how the model makes predictions.\n",
        "  * For example, in a linear regression model, the coefficients (weights) and intercept are parameters.\n",
        "  \n",
        "  The model adjusts these values to minimize error during training. Parameters are not manually set by the user; they are optimized automatically through learning algorithms such as gradient descent."
      ],
      "metadata": {
        "id": "6OInnj_QmGvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.2.What is correlation? What does negative correlation mean?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "  a) Correlation:\n",
        "\n",
        "   Correlation is a statistical measure that describes how strongly two variables are related to each other and in what direction.\n",
        "It tells us whether an increase or decrease in one variable will likely correspond to an increase or decrease in another.\n",
        "\n",
        "The correlation coefficient (denoted by r) ranges from –1 to +1:\n",
        "\n",
        "+ +1 → perfect positive correlation (both increase together)\n",
        "\n",
        "* 0 → no linear relationship\n",
        "\n",
        "* –1 → perfect negative correlation (one increases, the other decreases)\n",
        "\n",
        "Example:\n",
        "* If the temperature increases and ice-cream sales also increase, they have a positive correlation.\n",
        "\n",
        "\n",
        "b) **Negative Correlation:**\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n",
        "The correlation value lies between –1 and 0.\n",
        "\n",
        "The closer the value is to –1, the stronger the negative relationship.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* As the speed of a car increases, the time to reach the destination decreases.\n",
        "\n",
        "* As study time increases, number of mistakes on a test may decrease.\n",
        "\n",
        "* In short, negative correlation shows an inverse relationship between two variables.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8i428KoImha7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "Machine Learning (ML) is a branch of Artificial Intelligence (AI) that enables computers to learn patterns from data and make decisions or predictions without being explicitly programmed.\n",
        "Instead of following fixed rules, ML systems improve automatically through experience and exposure to more data.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Email systems use ML to classify messages as spam or not spam.\n",
        "\n",
        "* Netflix uses ML to recommend movies based on your watch history.\n",
        "\n",
        "\n",
        " **Main Components of Machine Learning**\n",
        "\n",
        "1. Data:\n",
        "The foundation of ML — includes input features and target labels.\n",
        "Example – Student marks, temperature readings, stock prices.\n",
        "\n",
        "2. Model:\n",
        "A mathematical representation that learns the relationship between input and output.\n",
        "Example – Linear Regression model, Decision Tree, Neural Network.\n",
        "\n",
        "3. Training:\n",
        "The process of feeding data to the model so it can adjust parameters and learn patterns.\n",
        "Example – The model “learns” which factors most affect house prices.\n",
        "\n",
        "4. Evaluation:\n",
        "Testing how well the trained model performs on unseen (test) data using metrics like accuracy, precision, or loss.\n",
        "\n",
        "5. Prediction (Inference):\n",
        "Using the trained model to make predictions on new data.\n",
        "Example – Predicting tomorrow’s weather using a trained weather model."
      ],
      "metadata": {
        "id": "F7RXj4lDo8XT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "ANSWERS-->\n",
        "The loss value (also called the cost function) is a numerical measure of how far the model’s predictions are from the actual target values.\n",
        "It tells us how well or poorly a model is performing during training.\n",
        "\n",
        "🔹 Key Points:\n",
        "\n",
        "A low loss value means the model’s predictions are close to the true values, indicating good performance.\n",
        "\n",
        "A high loss value means the model is making large errors and needs improvement.\n",
        "\n",
        "The model’s goal during training is to minimize this loss using optimization algorithms like Gradient Descent.\n",
        "\n",
        "Tracking loss over epochs helps detect overfitting or underfitting.\n",
        "\n",
        "Example:\n",
        "\n",
        "In Linear Regression, the loss function is often Mean Squared Error (MSE):\n",
        "\n",
        "  $$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "Example:\n",
        "\n",
        "- $y_i$ = actual value  \n",
        "- $\\hat{y}_i$ = predicted value  \n",
        "- $n$ = number of samples\n",
        "\n",
        "\n",
        "\n",
        "A smaller MSE means the model fits the data better.\n",
        "\n",
        " Example in Python:\n",
        "from sklearn.metrics import mean_squared_error\n",
        "loss = mean_squared_error(y_true, y_pred)\n",
        "print(\"Loss Value:\", loss)"
      ],
      "metadata": {
        "id": "HfJK2Dcuq9AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.5. What are continuous and categorical variables?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "- **Continuous Variables:**\n",
        "\n",
        "      Numeric values that can take any value within a range. Example – height, temperature, or salary.\n",
        "\n",
        "- **Categorical Variables:**\n",
        "\n",
        "      Represent categories or groups, not numbers. Example – gender (male/female), color (red/blue), or city name.\n",
        "      Continuous data is analyzed using mathematical operations, while categorical data often requires encoding before use in ML models."
      ],
      "metadata": {
        "id": "SnnM9fjGtqL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "ANSWERS-->\n",
        "\n",
        "Categorical variables are handled in machine learning by converting them into numerical representations, as most algorithms require numerical input. Common techniques include one-hot encoding, which creates new binary columns for each category, and label encoding, which assigns a unique integer to each category. Other methods like target encoding, frequency encoding, and binary encoding are also used depending on the data's characteritics.\n",
        "\n",
        "**Common techniques:**\n",
        "\n",
        "- Label Encoding: Assigns numeric values (e.g., Male = 0, Female = 1).\n",
        "\n",
        "- One-Hot Encoding: Creates binary columns for each category (e.g., “City_Mumbai”, “City_Delhi”).\n",
        "\n",
        "- Ordinal Encoding: Used when categories have order (e.g., Low = 1, Medium = 2, High = 3).\n",
        "\n",
        "Example:\n",
        "\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    ncoder = OneHotEncoder()\n",
        "    encoded = encoder.fit_transform(data[['City']])"
      ],
      "metadata": {
        "id": "4sAsSxLhukHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.7. What do you mean by training and testing a dataset?\n",
        "\n",
        "ANSWERS-->\n",
        "Training and testing a dataset are fundamental steps in machine learning, used to create and evaluate a predictive model. A single dataset is split into two or three subsets to teach an algorithm how to find patterns (training) and then assess its performance on new, unseen data (testing). This process is crucial for developing accurate, reliable models that can generalize to real-world scenarios.\n",
        "\n",
        "The process of training and testing\n",
        "\n",
        "- Splitting the data. The overall dataset is first divided into two or three parts:\n",
        "  - Training dataset: The largest portion of the data, typically 70–80%, is used to train the machine learning model. This data is fed to the algorithm so it can learn the underlying relationships and patterns between the input features and the target output.\n",
        "  -  Testing dataset: A smaller portion, typically 20–30%, is set aside and not used during the training phase. It provides an independent, unbiased final evaluation of the model's performance on new data.\n",
        "  - Validation dataset (optional): Some workflows also include a third dataset to fine-tune the model's hyperparameters and prevent overfitting to the training data. This is often used during the development phase before the final test.\n",
        "- Training the model. The model is given the training data to learn from. In supervised learning, this includes both the input data and the corresponding correct answers, or \"labels.\"\n",
        "  - For example, to train a spam filter, you would feed the model thousands of emails with the correct labels of \"spam\" or \"not spam\". The model adjusts its internal parameters and weights to minimize the difference between its predictions and the actual labels.\n",
        "\n",
        "- Testing the model. After training is complete, the model's performance is measured by having it make predictions on the testing dataset. Because the model has never seen this data before, the testing phase determines how well the model has learned to generalize its knowledge. The algorithm's predictions for the test data are then compared against the known, correct answers to calculate its accuracy and other performance metrics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jDvm3yzGvWJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.8. What is sklearn.preprocessing?\n",
        "\n",
        "ANSWERS-->\n",
        "\n",
        "sklearn.preprocessing is a module within the scikit-learn library in Python that provides a comprehensive set of tools for data preprocessing. Data preprocessing is a crucial step in machine learning, involving the transformation of raw data into a format suitable for training machine learning models. This is often necessary because many algorithms perform better with clean, scaled, or transformed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "yujRuQifwfOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.9. What is a Test set?\n",
        "The test set is a subset of the dataset reserved for evaluating the trained model’s performance. It helps determine whether the model can generalize well to unseen data.\n",
        "Example: After training a model on 80% of the data, the remaining 20% is used as a test set to compute accuracy or loss.\n",
        "It ensures that the model’s success isn’t due to memorization but real learning"
      ],
      "metadata": {
        "id": "eleeUbR1xysf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "ANSWER-->\n",
        "We use the train_test_split() function from sklearn.model_selection.\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "- Approach a Machine Learning problem\n",
        "\n",
        "Understand the problem and business goal.\n",
        "\n",
        "Collect and clean data by handling missing values and outliers.\n",
        "\n",
        "Perform EDA (Exploratory Data Analysis).\n",
        "\n",
        "Feature Engineering – create or transform features.\n",
        "\n",
        "Select an algorithm based on data type.\n",
        "\n",
        "Train the model and tune parameters.\n",
        "\n",
        "Evaluate performance on test data.\n",
        "\n",
        "Deploy and monitor the model.\n",
        "\n",
        "Example:\n",
        "- Predicting house prices involves these steps from gathering property data to evaluating accuracy.\n"
      ],
      "metadata": {
        "id": "CArHVY9sxyyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "Exploratory Data Analysis (EDA) helps us understand the dataset’s structure, detect missing values, outliers, and relationships among variables.\n",
        "It ensures the data is clean and meaningful before modeling.\n",
        "- For example, EDA might reveal that some columns are highly correlated or irrelevant, allowing better feature selection. Without EDA, the model may produce inaccurate results or overfit the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "tYyRi3DFxzM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.12. What is correlation?\n",
        "\n",
        "ANSWERS-->\n",
        "\n",
        " Correlation is a statistical measure that describes how strongly two variables are related to each other and in what direction. It tells us whether an increase or decrease in one variable will likely correspond to an increase or decrease in another.\n",
        "\n",
        "- The correlation coefficient (denoted by r) ranges from –1 to +1:\n",
        "\n",
        "- +1 → perfect positive correlation (both increase together)\n",
        "0 → no linear relationship\n",
        "\n",
        "- –1 → perfect negative correlation (one increases, the other decreases)"
      ],
      "metadata": {
        "id": "fn4qlHGNxzS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.13. What does negative correlation mean?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n",
        "The correlation value lies between –1 and 0.\n",
        "\n",
        "The closer the value is to –1, the stronger the negative relationship.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* As the speed of a car increases, the time to reach the destination decreases.\n",
        "\n",
        "* As study time increases, number of mistakes on a test may decrease.\n",
        "\n",
        "* In short, negative correlation shows an inverse relationship between two variables.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ywFVWRQQxzW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.14. How can you find correlation between variables in Python?\n",
        "\n",
        "ANSWER-->\n",
        "Using Pandas and Seaborn libraries:\n",
        "\n",
        "    import pandas as pd\n",
        "    import seaborn as sns\n",
        "    sns.heatmap(df.corr(), annot=True)\n",
        "\n",
        "\n",
        "df.corr() computes pairwise correlation, and the heatmap visually displays it.\n",
        "For instance, if two variables show r = 0.85, they are strongly positively correlated.\n",
        "\n"
      ],
      "metadata": {
        "id": "KdpOQE0vT-Fi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.15.  What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Causation means one variable directly affects another.\n",
        "\n",
        "Correlation means variables move together but one doesn’t necessarily cause the other.\n",
        "- Example: Ice cream sales and drowning cases are correlated because both rise in summer.\n",
        "\n",
        "However, ice cream doesn’t cause drowning — the underlying factor (temperature) causes both.\n",
        "Hence, correlation ≠ causation."
      ],
      "metadata": {
        "id": "3JzBX9qXUUh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "An optimizer adjusts model parameters to minimize loss during training.\n",
        "Common types:\n",
        "\n",
        "- SGD (Stochastic Gradient Descent): Updates weights after each sample.\n",
        "\n",
        "- Adam: Combines momentum and adaptive learning rates for faster convergence.\n",
        "\n",
        "- RMSProp: Works well for non-stationary data.\n",
        "\n",
        "Example:\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "Optimizers are crucial for improving accuracy and reducing training time."
      ],
      "metadata": {
        "id": "_bmcL1f_UhMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.17. What is sklearn.linear_model?\n",
        "\n",
        "sklearn.linear_model is a module in Scikit-learn that implements linear models such as:\n",
        "\n",
        "- LinearRegression() for continuous prediction\n",
        "\n",
        "- LogisticRegression() for classification\n",
        "\n",
        "- Ridge(), Lasso() for regularization\n",
        "Example:\n",
        "\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    model = LinearRegression()\n",
        "\n",
        "\n",
        "These models form the basis for many predictive ML tasks."
      ],
      "metadata": {
        "id": "9h6utn5TVM67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "model.fit() trains a machine learning model on the provided data.\n",
        "Syntax:\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "Here, X_train is the input data and y_train is the target variable. The function adjusts model parameters (like weights) to minimize the loss function based on the training data."
      ],
      "metadata": {
        "id": "LgqGAYg6WD5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "model.predict() is used to generate predictions on new or test data using the trained model.\n",
        "Example:\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "It requires only the feature inputs (X_test) and returns predicted outputs. The results can then be compared with actual labels to measure accuracy."
      ],
      "metadata": {
        "id": "rZ2cFOoeXTjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.20. What are continuous and categorical variables?\n",
        "\n",
        "ANSWER-->Variables (features) in datasets are types of data. Knowing the type helps you choose the right model and preprocessing.\n",
        "\n",
        "1. Continuous Variables\n",
        "\n",
        "Take numerical values that can be measured on a continuous scale.\n",
        "\n",
        "Often involve real numbers.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Age (e.g., 23, 23.5)\n",
        "\n",
        "- Temperature (e.g., 36.6°C)\n",
        "\n",
        "- House price (e.g., 250000.75)\n",
        "\n",
        "Can be used directly in most ML models.\n",
        "\n",
        "2. Categorical Variables\n",
        "\n",
        "Represent categories or groups, not numbers in a measurable sense.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Color: red, blue, green\n",
        "\n",
        "- Gender: male, female\n",
        "\n",
        "- Car type: SUV, sedan, hatchback\n",
        "\n",
        "Often need to be encoded into numbers for models (e.g., One-Hot Encoding, Label Encoding).\n",
        "\n",
        "Summary Table:\n",
        "\n",
        "| Feature Type | Example Values   | ML Notes                             |\n",
        "| ------------ | ---------------- | ------------------------------------ |\n",
        "| Continuous   | 23, 45.6, 100    | Use as-is; can calculate mean/std    |\n",
        "| Categorical  | red, blue, green | Encode before feeding to most models |odels"
      ],
      "metadata": {
        "id": "dy7h8apWXvo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.21What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "Feature Scaling standardizes or normalizes the range of independent variables so that no variable dominates due to its magnitude.\n",
        "It helps gradient-based algorithms (like logistic regression, SVM, neural networks) converge faster.\n",
        "Example:\n",
        "If one feature has values 1–10 and another 1,000–10,000, scaling makes both comparable, improving accuracy and performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IwCvTmRDfm1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e6jpOgJnf1GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.22. How do we perform scaling in Python?\n",
        "ANSWER-->\n",
        "  \n",
        "\n",
        "Using StandardScaler or MinMaxScaler from Scikit-learn:\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler() = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "StandardScaler: Transforms data to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "MinMaxScaler: Rescales data to a fixed range (usually 0–1).\n",
        "Scaling ensures all features contribute equally to the model."
      ],
      "metadata": {
        "id": "A0aDbMK2fZVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.23.What is sklearn.preprocessing?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "\n",
        "sklearn.preprocessing is a module within the scikit-learn library in Python that provides a comprehensive set of tools for data preprocessing. Data preprocessing is a crucial step in machine learning, involving the transformation of raw data into a format suitable for training machine learning models. This is often necessary because many algorithms perform better with clean, scaled, or transformed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "q_E1oO_LgNgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "qFMhlDbqf3mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary library\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Suppose X is your feature matrix and y is the target variable\n",
        "# Example:\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) # Example feature matrix\n",
        "y = np.array([0, 1, 0, 1, 0]) # Example target variable\n",
        "\n",
        "\n",
        "# Split data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,    # 20% data for testing\n",
        "    random_state=42,  # ensures reproducibility\n",
        "    shuffle=True      # shuffle data before splitting\n",
        ")\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMADw9NJguln",
        "outputId": "a0ad0765-b6fb-4639-dede-542b700ae005"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (4, 2)\n",
            "Testing data shape: (1, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q.25. Explain data encoding?\n",
        "\n",
        "ANSWER-->\n",
        "\n",
        "Data encoding is the process of converting categorical (non-numeric) or textual data into numerical values that a machine learning model can understand.\n",
        "\n",
        "Example:\n",
        "Suppose you have a feature Color with values: Red, Blue, Green. A model cannot understand strings like \"Red\" directly. Encoding converts them into numbers like:\n",
        "\n",
        "| Color | Encoded Value |\n",
        "| ----- | ------------- |\n",
        "| Red   | 0             |\n",
        "| Blue  | 1             |\n",
        "| Green | 2             |\n",
        "\n",
        "2. Types of Data Encoding\n",
        "a) Label Encoding\n",
        "\n",
        "Converts each category into a unique integer.\n",
        "\n",
        "Simple but may introduce unintended ordinal relationships (e.g., 0 < 1 < 2) even if categories are not ordered.\n",
        "\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    colors = ['Red', 'Blue', 'Green', 'Blue']\n",
        "    encoder = LabelEncoder()\n",
        "    encoded_colors = encoder.fit_transform(colors)\n",
        "\n",
        "    print(encoded_colors)  # Output: [2 0 1 0] (order may vary)\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "b) One-Hot Encoding\n",
        "\n",
        "Converts each category into a binary vector (0 or 1).\n",
        "\n",
        "Prevents the model from assuming any order among categories.\n",
        "\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    import numpy as np\n",
        "\n",
        "    colors = np.array(['Red', 'Blue', 'Green', 'Blue']).reshape(-1, 1)\n",
        "    encoder = OneHotEncoder(sparse=False)\n",
        "    encoded_colors = encoder.fit_transform(colors)\n",
        "\n",
        "    print(encoded_colors)\n",
        "\n",
        "\n",
        "Output:\n",
        "\n",
        "| Red | Blue | Green |\n",
        "| --- | ---- | ----- |\n",
        "| 1   | 0    | 0     |\n",
        "| 0   | 1    | 0     |\n",
        "| 0   | 0    | 1     |\n",
        "| 0   | 1    | 0     |\n",
        "\n",
        "\n",
        " When to use: For nominal categorical data (no order).\n",
        "\n",
        "c) Ordinal Encoding\n",
        "\n",
        "Similar to label encoding but explicitly respects the order.\n",
        "\n",
        "Example: Size: Small < Medium < Large → [0, 1, 2].\n",
        "\n",
        "    from sklearn.preprocessing import OrdinalEncoder\n",
        "    sizes = [['Small'], ['Medium'], ['Large'], ['Medium']]\n",
        "    encoder = OrdinalEncoder(categories=[['Small','Medium','Large']])\n",
        "    encoded_sizes = encoder.fit_transform(sizes)\n",
        "\n",
        "print(encoded_sizes)\n",
        "\n",
        "d) Other Encodings\n",
        "\n",
        "Binary Encoding: Converts categories into binary numbers (useful for high-cardinality data).\n",
        "\n",
        "Frequency/Count Encoding: Uses the frequency of each category as its numerical value.\n",
        "\n",
        "Target Encoding: Uses the mean of the target variable for each category (mostly in supervised learning).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fHdbtEEIg_Dw"
      }
    }
  ]
}